{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad25058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packagesfrom segment_anything import SamPredictor, sam_model_registry\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import torch\n",
    "from segment_anything import sam_model_registry\n",
    "import cv2 \n",
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22eab6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the Segment Anything Model\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "CHECKPOINT_PATH=\"/Users/nirwantandukar/Documents/trial_BZea/sam_vit_h_4b8939.pth\"\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH)\n",
    "sam.to(device=DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03eb9c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated Mask (Instance Segmentation) Generation with SAM\n",
    "# segmentation - [np.ndarray] - the mask with (W, H) shape, and bool type, where W and H are the width and height \n",
    "#of the original image, respectively\n",
    "#area - [int] - the area of the mask in pixels\n",
    "#bbox - [List[int]] - the boundary box detection in xywh format\n",
    "#predicted_iou - [float] - the model's own prediction for the quality of the mask\n",
    "#point_coords - [List[List[float]]] - the sampled input point that generated this mask\n",
    "#stability_score - [float] - an additional measure of mask quality\n",
    "#crop_box - List[int] - the crop of the image used to generate this mask in xywh format\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "IMAGE_PATH = \"maize.jpeg\"\n",
    "image_bgr = cv2.imread(IMAGE_PATH)\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78693382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image = cv2.imread('maize.jpeg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0e4d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Image Compression using Gaussian Pyramid\n",
    "G = image.copy()\n",
    "compressed_image = [G]\n",
    "for i in range(6):\n",
    " G = cv2.pyrDown(G)\n",
    " compressed_image.append(G)\n",
    "\n",
    "#compressed_image = cv2.pyrDown(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f9d526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71dc0025",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) :-1: error: (-5:Bad argument) in function 'cvtColor'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 2. Background Separation\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Convert to LAB color space\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m lab_image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompressed_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2Lab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m l_channel, a_channel, b_channel \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39msplit(lab_image)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Thresholding on the blue channel to segment maize kernels\u001b[39;00m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) :-1: error: (-5:Bad argument) in function 'cvtColor'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n"
     ]
    }
   ],
   "source": [
    "# 2. Background Separation\n",
    "# Convert to LAB color space\n",
    "lab_image = cv2.cvtColor(compressed_image, cv2.COLOR_BGR2Lab)\n",
    "l_channel, a_channel, b_channel = cv2.split(lab_image)\n",
    "\n",
    "# Thresholding on the blue channel to segment maize kernels\n",
    "_, thresh = cv2.threshold(b_channel, 128, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "#cv2.imshow('Processed Image', lab_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc07364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Mean Shift Filtering\n",
    "# Parameters for mean shift\n",
    "sp = 40  # spatial window radius\n",
    "sr = 60  # color window radius\n",
    "mean_shift_filtered = cv2.pyrMeanShiftFiltering(compressed_image, sp, sr, maxLevel=3)\n",
    "cv2.imshow('Processed Image', mean_shift_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "574076a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Enhancement of Kernel Edges using Colour Deconvolution\n",
    "# Convert to LAB color space\n",
    "lab_filtered = cv2.cvtColor(mean_shift_filtered, cv2.COLOR_BGR2Lab)\n",
    "l_channel, a_channel, b_channel = cv2.split(lab_filtered)\n",
    "\n",
    "# Define OD matrices for the three stain combinations\n",
    "methyl_green = np.array([0.98003, 0.144316, 0.133146])\n",
    "hematoxylin_gl = np.array([0.644211, 0.716556, 0.266844])\n",
    "white = np.array([0, 0, 0])\n",
    "\n",
    "# Choose the appropriate stain combination based on the average LAB values\n",
    "avg_l = np.mean(l_channel)\n",
    "avg_a = np.mean(a_channel)\n",
    "avg_b = np.mean(b_channel)\n",
    "\n",
    "# Calculate Euclidean distances for each stain combination\n",
    "dist_methyl_green = np.linalg.norm(methyl_green - np.array([avg_l, avg_a, avg_b]))\n",
    "dist_hematoxylin_gl = np.linalg.norm(hematoxylin_gl - np.array([avg_l, avg_a, avg_b]))\n",
    "dist_white = np.linalg.norm(white - np.array([avg_l, avg_a, avg_b]))\n",
    "\n",
    "# Choose the stain with the minimum distance\n",
    "if min(dist_methyl_green, dist_hematoxylin_gl, dist_white) == dist_methyl_green:\n",
    "    chosen_stain = methyl_green\n",
    "elif min(dist_methyl_green, dist_hematoxylin_gl, dist_white) == dist_hematoxylin_gl:\n",
    "    chosen_stain = hematoxylin_gl\n",
    "else:\n",
    "    chosen_stain = white\n",
    "\n",
    "# Apply Colour Deconvolution\n",
    "# Convert the LAB image to a linear scale\n",
    "linear_lab = np.exp(-1.0 * lab_filtered / 255.0)\n",
    "# Apply the chosen stain\n",
    "deconvoluted = linear_lab * chosen_stain\n",
    "\n",
    "# Convert back to 8-bit image\n",
    "deconvoluted = (deconvoluted * 255).astype(np.uint8)\n",
    "\n",
    "# Display the processed image\n",
    "#cv2.imshow('Processed Image', deconvoluted)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a1458a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel Segmentation\n",
    "# Convert the processed image to grayscale\n",
    "gray_image = cv2.cvtColor(deconvoluted, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Otsu's thresholding\n",
    "_, otsu_thresh = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "# Adaptive thresholding\n",
    "block_size = 25  # This should be set to the average length of kernels MBR\n",
    "adaptive_thresh = cv2.adaptiveThreshold(gray_image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, block_size, 2)\n",
    "\n",
    "# Kernel Recognition\n",
    "# Step 1: Smoothing the kernel binary image with Gaussian filter\n",
    "kernel_size = 5  # This should be set to the average width of seed MBR\n",
    "smoothed_image = cv2.GaussianBlur(adaptive_thresh, (kernel_size, kernel_size), 0)\n",
    "\n",
    "# Step 2: Local maximum detection\n",
    "local_maxima = cv2.dilate(smoothed_image, np.ones((kernel_size, kernel_size), np.uint8))\n",
    "local_maxima = cv2.compare(smoothed_image, local_maxima, cv2.CMP_EQ)\n",
    "\n",
    "# Step 3: Eliminate spurious kernel recognition points\n",
    "# Find contours in the local_maxima image\n",
    "contours, _ = cv2.findContours(local_maxima, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "for contour in contours:\n",
    "    if cv2.contourArea(contour) < 5:  # This threshold can be adjusted\n",
    "        cv2.drawContours(local_maxima, [contour], 0, 0, -1)\n",
    "\n",
    "# Display the results\n",
    "cv2.imshow('Otsu Thresholding', otsu_thresh)\n",
    "cv2.imshow('Adaptive Thresholding', adaptive_thresh)\n",
    "cv2.imshow('Kernel Recognition', local_maxima)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bf9c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming DPI is 300 (change this to your actual DPI)\n",
    "dpi = 144\n",
    "# Calculate the number of pixels to trim from each side\n",
    "trim_pixels = 1 * dpi\n",
    "# Trim the image\n",
    "trimmed_image_rgb = image_rgb[trim_pixels:-trim_pixels, trim_pixels:-trim_pixels]\n",
    "\n",
    "# Save the trimmed image if you want\n",
    "#cv2.imwrite('/path/to/save/trimmed_image.jpg', cv2.cvtColor(trimmed_image_rgb, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "\n",
    "result = mask_generator.generate(trimmed_image_rgb)\n",
    "\n",
    "# Sort the result by area in descending order\n",
    "sorted_result = sorted(result, key=lambda x: x['area'], reverse=True)\n",
    "\n",
    "# Take the top 20 largest area masks\n",
    "top_20_result = sorted_result[:30]\n",
    "\n",
    "# Iterate through the list of top 20 segments\n",
    "for i, segment in enumerate(top_20_result):\n",
    "    # Convert boolean mask to uint8\n",
    "    mask_uint8 = np.uint8(segment['segmentation'] * 255)\n",
    "    \n",
    "    # Show the mask\n",
    "    cv2.imshow(f'Top 20 Generated Mask {i+1}', mask_uint8)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
